<!DOCTYPE html>
<html lang="en-us"><head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<meta name="generator" content="Hugo 0.88.1" />
	
	<link rel="icon" href="/images/logo.png">
	
	<title>Introduction to Machine Translation in NLP | Chandan&#39;s Blog</title>
	
	

	<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://chandan5362.github.io/blog/machine-translation/cover.jpg"/>
<meta name="twitter:title" content="Introduction to Machine Translation in NLP"/>
<meta name="twitter:description" content="Imagine yourself in 1800s and you plan a travel to France being a native english speaker. It would have been nearly impossible to communicate with native French speaking community."/>

	<meta property="og:title" content="Introduction to Machine Translation in NLP" />
<meta property="og:description" content="Imagine yourself in 1800s and you plan a travel to France being a native english speaker. It would have been nearly impossible to communicate with native French speaking community." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chandan5362.github.io/blog/machine-translation/" /><meta property="og:image" content="https://chandan5362.github.io/blog/machine-translation/cover.jpg"/><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2021-10-09T22:43:36+05:30" />
<meta property="article:modified_time" content="2021-10-09T22:43:36+05:30" />


	
<script async src="https://www.googletagmanager.com/gtag/js?id=G-DNTXE8FETV"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-DNTXE8FETV', { 'anonymize_ip': false });
}
</script>

	<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">

	
	<link rel="stylesheet" href="https://chandan5362.github.io/css/medium.35a76276708fbb27d28e4a4423c0531cc4397aba47f5f3c2d4b89f1f881132dd.css" integrity="sha256-NadidnCPuyfSjkpEI8BTHMQ5erpH9fPC1LifH4gRMt0=">

	
	<link rel="stylesheet" href="https://chandan5362.github.io/css/additional.8819b6defcdc6d21280f9b402b00df87ca779135901de6c22e708c62e20184b9.css" integrity="sha256-iBm23vzcbSEoD5tAKwDfh8p3kTWQHebCLnCMYuIBhLk=">

	
	
	<script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  
    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function(x){
          x.parentElement.classList += 'has-jax'})
      });
  
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">
    <div class="container pr-0">
        
        <a class="navbar-brand" href="https://chandan5362.github.io//">

            
            <img src="/images/logo.png" alt="logo">
            
        </a>
        

        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent"
            aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        
        <div class="collapse navbar-collapse" id="navbarMediumish">
            
            <ul class="navbar-nav ml-auto">
                 
                <li class="nav-item ">
                    <a class="nav-link" href="/blog">Blog</a>
                </li>
                 
                <li class="nav-item ">
                    <a class="nav-link" href="/">About Me</a>
                </li>
                 
                <li class="nav-item ">
                    <a class="nav-link" href="/glossary">ML Glossary</a>
                </li>
                
            </ul>
        </div>
        
    </div>
</nav>


        <div class="site-content">   
            <div class="container">
<div class="mainheading">
    <h1 class="sitetitle">Chandan&#39;s Blog</h1>
    <p class="lead">
         
    </p>
</div><div class="main-content">
        
        <div class="container">
            <div class="row">
                
                <div class="col-md-2 pl-0"><div class="share sticky-top sticky-top-offset">
    <p>Share</p>
    <ul>
        <li class="ml-1 mr-1">
        <a target="_blank" href="https://twitter.com/intent/tweet?text=Introduction%20to%20Machine%20Translation%20in%20NLP&url=https%3a%2f%2fchandan5362.github.io%2fblog%2fmachine-translation%2f" onclick="window.open(this.href, 'twitter-share', 'width=550,height=435');return false;">
        <i class="fab fa-twitter"></i>
        </a>
        </li>
        
        <li class="ml-1 mr-1">
        <a target="_blank" href="https://facebook.com/sharer.php?u=https%3a%2f%2fchandan5362.github.io%2fblog%2fmachine-translation%2f" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;">
        <i class="fab fa-facebook-f"></i>
        </a>
        </li>

        <li class="ml-1 mr-1">
        <a target="_blank" href="https://www.xing.com/spi/shares/new?url=https%3a%2f%2fchandan5362.github.io%2fblog%2fmachine-translation%2f" onclick="window.open(this.href, 'xing-share', 'width=550,height=435');return false;">
        <i class="fab fa-xing"></i>
        </a>
        </li>        
    </ul>

    
</div>
</div>
                                
                <div class="col-md-9 flex-first flex-md-unordered">
                    <div class="mainheading">
                        	
                        
                        
                        
                        <div class="row post-top-meta">
                            <div class="col-xs-12 col-md-3 col-lg-2 text-center text-md-left mb-4 mb-md-0 md-nopad-right">
                                <img class="author-thumb" src="/images/author.png" alt="Chandan Kumar Roy">
                            </div>
                            <div class="col-xs-12 col-md-9 col-lg-10 text-center text-md-left md-nopad-left">
                                <a target="_blank" class="link-dark">Chandan Kumar Roy</a><br>
                                <span class="author-description">
                                    Author of this blog.<br>
                                    <i class="far fa-star"></i>
                                    Oct 9, 2021
                                    <i class="far fa-clock clock"></i>
                                    8 min read
                                </span>					
                            </div>
                        </div>			
                        	
                        
                                                
                        
                        <h1 class="posttitle">Introduction to Machine Translation in NLP</h1> 
                    </div>

                    
                    
                    
                        <img class="featured-image img-fluid" src="https://chandan5362.github.io/blog/machine-translation/cover.jpg" alt="thumbnail for this post">
                    
                    

                    
                    <div class="article-post">
                        <p>Imagine yourself in 1800s and you plan a travel to France being a native english speaker. It would have been nearly impossible to communicate with native French speaking community. But now the world of impossibilities has been simplified with the technologies and research that have gained pace in the past couple of decades.  <strong>Natural Language Processing</strong> is a subfield of <strong>AI</strong> and <strong>Linguistic</strong> that deals machine interaction with human language.</p>
<p><strong>Machine Translation(MT)</strong> is one of the many applications of NLP. It is automatically translates one language to others using some algorithms under the hood. We will see them shortly. In the early days of NLP, engineers used to hard code the translation into dictionary using key-value pairs and then they used to follow certain rules for translating source languages to their respective counterpart. They were know as <a href="https://en.wikipedia.org/wiki/Rule-based_machine_translation"><strong>Rule-based Machine Translation(RBMT)</strong></a>. But they could only handle a limited amount of word pairs into databases. Also increaing those limits used to come at the cost of time and money. Then came the probablistic models  also known as <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwixo77M3b_zAhWBzjgGHaDRBDQQFnoECAgQAw&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FStatistical_machine_translation&amp;usg=AOvVaw0ZZqUPekMdlGaGSWrHZNh3"><strong>Statistical Machine Translation(SMT)</strong></a> that were more flexible in learning and translating languages and were based on bilingual text corpora. it was first introduced in 1955 but only gained recognitition after the establishment of IBM watson resarch center. I will cover more about SMT in another blog. By the end of 2010, researchers became more interested in  deep learning techniques and high quality translation became seemingly  possible due to  availabiltiy of cost-friendly computaional resources (GPUs). Though deep learning appeared first in speech recognition in 1990s but the first paper on <strong><a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural Machine Translation(NMT)</a></strong> was published in 2014. After that focus of researchers shifted comletely towards NMT as they were producing high quality translation and multi-lingual translation also became possible after the arrival of NMT.</p>
<p>In this post, I will mainly be covering about how one can approach a machine translation in terms of basic machine learning algorithms.  Let&rsquo;s see how it works.</p>
<h1 id="how-machine-translation-works">How Machine Translation works?</h1>
<p>Now imagine that you are planning a travel to France and you don&rsquo;t want to spend money on hiring local guides to take you around (<em>let&rsquo;s keep the smartphone out of context for now</em>). So you came up with an idea to build your own machine translation device. It might seem impossible to think at first but right there, I came as a saviour and asked you to follow my words carefully (<em><strong>Attention Is All You Need</strong></em> right now).</p>
<p>Let&rsquo;s begin our basic MT building journey with an understanding of how you should approach it.</p>
<p align="center" >
  <img src="mt1.png"/>
  <figcaption  align="center" style='font-style:italic; color:black;'>source: References[1] </figcaption>
</p>
<p>In the above picture, you can see that how a english word <code>cat</code> can be translated to its french equivalent <code>chat</code>.  In the left-hand side of the image, words are represented as numbers called as <em><strong>vectors</strong></em>. These vectors have a special name in NLP that in <em><strong>word-embeddings</strong></em>. Words can be translated into their lower dimensional vector representation using number of techniques  like <code>word2vec</code> , <code>Latent Semantic Analysis encoding</code>, <code>TF-IDF encoding</code> etc. Right-hand side of the image represents the  french words embeddings.</p>
<hr>
<p><em>In eng-french context, Machine translation finds a similar french word embedding corresponding to each word embedding in English language</em>.</p>
<hr>
<p>It means that to find a french translation of word <code>cat</code> in English, If you are able to find a vector ( among French word embeddings) similar to cat&rsquo;s embeddings, then that embedding would correspond to word <code>chat</code> in French.  But the question arises, how to find similar french embeddings for english words? Well, from here, we will proceed mathematically. Let&rsquo;s our problem statement speak the language of typical Machine Learning.</p>
<p>Let $X$ be a row vector represention of English words and $Y$ be its french counterpart  row vector representation. Then MT problem tries to find a transformation matrix $R$  that satisfies the following equation.
$$
XR â‰ˆ Y
$$
There are still couple of questions to be answered. They are as under.</p>
<ol>
<li>How do you find transformation matrix $R$ ?</li>
<li>What does similarity of word-embeddings of English language wrt. French word mean?</li>
</ol>
<p>Well, I will be answering these questions one by one in subsequent paragraphs.</p>
<h2 id="how-to-find-transformation-matrix-r">How to find transformation matrix R?</h2>
<p>It can be answered in terms of optimization problem. Select a random matrix R (<em>just like weight initialization in neural networks</em>) and then try to minimise the distance between $XR$ and $Y$ (Both $X$ and $Y$ are row vectors), as simple as that.  Now is the time to define our optimzation problem mathematically.</p>
<p align="center" >
  <img src="mt2.png"/>
  <figcaption  align="center" style='font-style:italic; color:black;'>source: References[1] </figcaption>
</p>
<p><em>It might come to your mind that why do you need to represent word vectors in terms of matrix even though it is possible to store them using dictionary as key-value pair. Well, you are right but how about any unseen word that is not a key of the  french-english dictionary. One of the solution is to find a robust model that can also work for unseen word. Now you know why do we need a matrix representation and their transformation matrix.  We only need to train our MT model on subset of English-french word dictionary and our model can easily predict the corresponding french equivalent for an unseen english word.</em></p>
<h5 id="mt-as-an-optimization-problem">MT as an Optimization problem:</h5>
<p>Find a transformation matrix $R$  that minimises the following equation.
$$
\arg \min _{\mathbf{R}}||\mathbf{XR}- \mathbf{Y}||_F \tag{1}
$$
Usually the loss fucntion is the squared <em><strong><a href="https://www.geeksforgeeks.org/finding-the-frobenius-norm-of-a-given-matrix/">Frobenius norm</a></strong></em> of <em>equation (1)</em> as it becomes quite handy while calculating gradient.</p>
<h5 id="loss-function">Loss Function:</h5>
<p>$$
L = ||Y-XR||_F^2
$$</p>
<p>where $F$ is the frobenius norm of $L$. So our loss function is the square of frobenius norm of L (<em>difference between French equivalent matrix $Y$ and $XR$</em>).</p>
<h4 id="gradient-computation-wrt-transformation-r">Gradient computation wrt. Transformation R</h4>
<p>We can use gradient descent to minimise the loss matix L. Let&rsquo;s see  how can we do that.
$$
\begin{align*}Loss(L) = ||Y-XR||_F^2
\newline
\newline
\frac{dL}{dR} = \frac{2}{m}(X^T(XR-Y))\end{align*}
$$</p>
<p>Remember that the transformed vector $á»¸$ wont be necessarily similar to $Y$. Therefore we need to find the closest match of  $á»¸$ in $Y$. That can be achieved using k-nearest neighbour.  Naive k-nearest neighbour is time consuming as it computes the similarity score with respect to every word vectors in $Y$. So Instead of naive KNN, we  will use nearest neigbour approximation using <strong>Locality Sensitivity Hashing</strong>. Everything so far so good, but how does LSH works?</p>
<h2 id="locality-sensitivity-hashing">Locality Sensitivity Hashing</h2>
<p>It is a hashing method based on the location of words in vector space. It is used to reduce the cost of naive KNN in higher dimensios. To understand Locality Sensitivity Hashing,  you need to know about Hashing data structure. You can refer to this post to learn more about <a href="https://www.geeksforgeeks.org/hashing-data-structure/">hashing</a>. Assuming that you already know hashing, let&rsquo;s see how LSH works.</p>
<p>LSH buckets the similar vector in high dimension with high probability. LSH works slightly different than normal hashing.</p>
<p align="center" >
  <img src="mt3.png"/>
  <figcaption  align="center" style='font-style:italic; color:black;'>source: References[1] </figcaption>
</p>
<p>In the above figure we can see that we LSH tries to find the plane that separates the nearest vecrors from the distant vectors.</p>
<p>Workig of LSH:</p>
<ul>
<li>
<p>It first find the separating hyperplane that groups similar word in vector space.</p>
</li>
<li>
<p>given the equation of plane $P$ , find the sign of word vctor $V$ using dot product
$$
sign = sgn(||P.V^T||)
$$
where $sgn$ is <em><a href="https://en.wikipedia.org/wiki/Sign_function">signum</a></em> function. if <em>sign</em> is 1, vector lies above the plane and it&rsquo;s dot product indicates the projection of vector $V$ on the plane on positive side (above the plane). if the <em>sign</em> is -1, vector lies below the plane and if the <em>sign</em> is 0, given word vector $V$ lies on the plane $P$.  It might sound little confusing. Let&rsquo;s see these concepts with a diagram.</p>
<p align="center" >
  <img src="mt4.png" height="350" width="400"/>
</p>
<p>In the given image,  blue line represents the vector $P$ normal to the separating plane (in black). Blue points represent those vector thta lies above the plane (in black, $PV_i^T$ &gt; 0) and those in red lie below the plane (dot product wth respect to normal vector(blue line) is negative).</p>
<p>We can combine multiple such planes in one hash fucntion using multiplane hashing heuristic as given below.</p>
<p>for a given plane $p_i $  and word vector $v_i$, heuristic $h_i$ is given as
$$
\begin{align*}
h_i:= \begin{cases}
sgn(p_iv_i^T) &amp; \text{if }p_iv_i^T &gt; 0,\newline
1+sgn(p_iv_i^T) &amp; \text{if }p_i{v_i}^T &gt; 0.
\end{cases}
\end{align*}
$$</p>
</li>
<li>
<p>Find the hash value using hash function as under.
$$
hashvalue = Î£_i^H 2^ih_i
$$</p>
</li>
</ul>
<p>The idea behind the <em>locality sensitivity hashing</em> is to divide the vector space into multiple regions and then select the approximate(friendly) nearest neighbours of $á»¸$  in $Y$ . This process of approximate k-nearest neighbours is faster than the naive KNN as the latter is more time and memory consuming. A-KNN(approximate k-nearest neighbour) does not give you the fullest nearest neighbour. it usuallly trades off accuracy over efficiency. ppAroxiumate nearest neighbours can also be used for document searching.</p>
<p>You can use cosine similarity score to compute the nearest neighbours. Cosine similarity score is given by
$$
\cos(u,v)=\frac{u\cdot v}{\left|u\right|\left|v\right|}
$$</p>
<ul>
<li>if <em>cos(u,v) = 1</em>, then u,v lies on the same side.</li>
<li>if <em>cos(u, v) = -1</em>, then u,v lies on the opposite side.</li>
<li>if <em>cos(u,v) = 0</em>, them u,v lies on the same plane.</li>
</ul>
<p>Summarising everything, we first find the word embeddings for English-french dictionary. Once we have the embeddings for every word, we find a transformation matrix that minimises the frobenius norm of $||XR-Y||$. Then we find the nearest vector $v_i$ in $Y$ That is near to $XR$ using LSH . We use LSH for faster nearest neighbor approximation. That&rsquo;s all for now.</p>
<p><strong><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></strong> in bold letter changed the face of NLP. Do chekout the paper.</p>
<p>Stay tuned for more interesting blogs about NLP.</p>
<p>Happy Learning&hellip;ðŸ“šðŸ“–ðŸ™‡</p>
<h1 id="references">References</h1>
<ol>
<li>image credits: Natural Language processing with classification and vector space coursera course. (Please do checkout the coursera course for in-depth intuition)</li>
<li>cover image: <a href="https://www.pexels.com">https://www.pexels.com</a></li>
</ol>

                    </div>
                    
                    
                    <div class="after-post-tags">
                        <ul class="tags">
                        
                        <li>
                        <a href="/tags/natural-language-processing">Natural Language Processing</a>
                        </li>
                        
                        <li>
                        <a href="/tags/machine-learning">Machine Learning</a>
                        </li>
                        
                        </ul>
                    </div>
                    
                    
                    
                    <div class="row PageNavigation d-flex justify-content-between font-weight-bold">
                    
                        <a class="d-block col-md-6" href="https://chandan5362.github.io/blog/sentence-autocompletion-using-n-gram-language-model/"> &laquo; Sentence Autocompletion Using N-Gram Language Model</a>
                    
                    
                        <a class="d-block col-md-6 text-lg-right" href="https://chandan5362.github.io/blog/hypothesis-testing/">Hypothesis Testing &raquo;</a>
                    
                    <div class="clearfix"></div>
                    </div>
                    
                </div>
                
            </div>
        </div>
        
        
    </div>


            </div>
<div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
			<div class="d-md-flex align-items-center justify-content-center h-100">
				<h2 class="d-md-block d-none align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">â†’</span></h2>
			</div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">
			
			<a class="mt-1 mb-1" href="/tags/data-science">data-science</a>
			
			<a class="mt-1 mb-1" href="/tags/linear-algebra">linear-algebra</a>
			
			<a class="mt-1 mb-1" href="/tags/machine-learning">machine-learning</a>
			
			<a class="mt-1 mb-1" href="/tags/natural-language-processing">natural-language-processing</a>
			
			<a class="mt-1 mb-1" href="/tags/statistics">statistics</a>
			
		</div>
	</div>
</div>

<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                &copy; Copyright Chandan Roy - All rights reserved
            </div>
            <div class="col-md-6 col-sm-6 text-center text-lg-right">    
                <a target="_blank" rel="noopener" href="https://www.wowthemes.net">Mediumish Theme</a> by WowThemes.net
            </div>
        </div>
    </div>

      
</footer>




        </div>


<script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>


<script src="https://chandan5362.github.io/js/mediumish.84218587c174fd40bce82544b98851670f0b124a7324b349c54a4065e2b32ffc.js" integrity="sha256-hCGFh8F0/UC86CVEuYhRZw8LEkpzJLNJxUpAZeKzL/w="></script>
    </body>
</html>
